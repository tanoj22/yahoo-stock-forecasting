{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b9cbf93-c8cf-4f43-bd9f-bd2626997478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root on sys.path: C:\\Users\\saita\\Yahoo Stock Forecasting\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path.cwd()\n",
    "if ROOT.name.lower() == \"notebooks\":  \n",
    "    ROOT = ROOT.parent\n",
    "\n",
    "sys.path.insert(0, str(ROOT))\n",
    "print(\"Project root on sys.path:\", ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "015e401d-02e7-4c86-ad1c-857a4d414db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from src.config import (\n",
    "    LSTM_LOOKBACK,\n",
    "    LSTM_HIDDEN_SIZE,\n",
    "    LSTM_NUM_LAYERS,\n",
    "    LSTM_DROPOUT,\n",
    "    LSTM_BATCH_SIZE,\n",
    "    LSTM_NUM_EPOCHS,\n",
    "    LSTM_LR,\n",
    "    MODEL_DIR,\n",
    ")\n",
    "from src.models_lstm import LSTMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ae78f45-5adb-4d4b-9d83-cd67c2565151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (1217, 60, 6)\n",
      "y_train: (1217, 1)\n",
      "X_val  : (213, 60, 6)\n",
      "y_val  : (213, 1)\n",
      "X_test : (215, 60, 6)\n",
      "y_test : (215, 1)\n",
      "Sequence length: 60 | Num features: 6\n"
     ]
    }
   ],
   "source": [
    "data = np.load(\"lstm_data.npz\")\n",
    "\n",
    "X_train = data[\"X_train\"]\n",
    "y_train = data[\"y_train\"]\n",
    "X_val   = data[\"X_val\"]\n",
    "y_val   = data[\"y_val\"]\n",
    "X_test  = data[\"X_test\"]\n",
    "y_test  = data[\"y_test\"]\n",
    "\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"X_val  :\", X_val.shape)\n",
    "print(\"y_val  :\", y_val.shape)\n",
    "print(\"X_test :\", X_test.shape)\n",
    "print(\"y_test :\", y_test.shape)\n",
    "\n",
    "num_features = X_train.shape[-1]\n",
    "seq_len = X_train.shape[1]\n",
    "print(\"Sequence length:\", seq_len, \"| Num features:\", num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "119e2bf3-4aa3-4f85-8c56-3c75cd9ab622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "train_ds = TensorDataset(\n",
    "    torch.tensor(X_train, dtype=torch.float32),\n",
    "    torch.tensor(y_train, dtype=torch.float32),\n",
    ")\n",
    "\n",
    "val_ds = TensorDataset(\n",
    "    torch.tensor(X_val, dtype=torch.float32),\n",
    "    torch.tensor(y_val, dtype=torch.float32),\n",
    ")\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=LSTM_BATCH_SIZE, shuffle=True)\n",
    "val_dl   = DataLoader(val_ds,   batch_size=LSTM_BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ce65120-2462-476c-a8af-aa45d7994ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMRegressor(\n",
      "  (lstm): LSTM(6, 64, num_layers=2, batch_first=True, dropout=0.2)\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "Model will be saved to: C:\\Users\\saita\\Yahoo Stock Forecasting\\models\\lstm_model_best.pth\n",
      "Early stopping: patience=7, min_delta=0.0001\n"
     ]
    }
   ],
   "source": [
    "model = LSTMRegressor(\n",
    "    num_features=num_features,\n",
    "    hidden_size=LSTM_HIDDEN_SIZE,\n",
    "    num_layers=LSTM_NUM_LAYERS,\n",
    "    dropout=LSTM_DROPOUT,\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LSTM_LR)\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "best_path = os.path.join(MODEL_DIR, \"lstm_model_best.pth\")\n",
    "\n",
    "PATIENCE = 7        \n",
    "MIN_DELTA = 1e-4   \n",
    "\n",
    "print(model)\n",
    "print(\"Model will be saved to:\", best_path)\n",
    "print(f\"Early stopping: patience={PATIENCE}, min_delta={MIN_DELTA}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18a7c6fa-0a48-459e-aafe-2b574fb83de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | train_loss=0.190906 | val_loss=0.125987\n",
      "  -> New best model saved (val_loss=0.125987)\n",
      "Epoch 002 | train_loss=0.023672 | val_loss=0.028719\n",
      "  -> New best model saved (val_loss=0.028719)\n",
      "Epoch 003 | train_loss=0.006938 | val_loss=0.003209\n",
      "  -> New best model saved (val_loss=0.003209)\n",
      "Epoch 004 | train_loss=0.002119 | val_loss=0.000996\n",
      "  -> New best model saved (val_loss=0.000996)\n",
      "Epoch 005 | train_loss=0.002315 | val_loss=0.001970\n",
      "  -> No improvement for 1 epoch(s)\n",
      "Epoch 006 | train_loss=0.001548 | val_loss=0.001831\n",
      "  -> No improvement for 2 epoch(s)\n",
      "Epoch 007 | train_loss=0.001422 | val_loss=0.002170\n",
      "  -> No improvement for 3 epoch(s)\n",
      "Epoch 008 | train_loss=0.001404 | val_loss=0.002392\n",
      "  -> No improvement for 4 epoch(s)\n",
      "Epoch 009 | train_loss=0.001327 | val_loss=0.003132\n",
      "  -> No improvement for 5 epoch(s)\n",
      "Epoch 010 | train_loss=0.001370 | val_loss=0.006955\n",
      "  -> No improvement for 6 epoch(s)\n",
      "Epoch 011 | train_loss=0.001287 | val_loss=0.003963\n",
      "  -> No improvement for 7 epoch(s)\n",
      "\n",
      "Early stopping triggered after 11 epochs.\n"
     ]
    }
   ],
   "source": [
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in range(1, LSTM_NUM_EPOCHS + 1):\n",
    "    # ---- Training ----\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for xb, yb in train_dl:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(xb)          # (batch, 1)\n",
    "        loss = criterion(preds, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * xb.size(0)\n",
    "\n",
    "    train_loss /= len(train_dl.dataset)\n",
    "\n",
    "    # ---- Validation ----\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_dl:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "\n",
    "            val_loss += loss.item() * xb.size(0)\n",
    "\n",
    "    val_loss /= len(val_dl.dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch:03d} | train_loss={train_loss:.6f} | val_loss={val_loss:.6f}\")\n",
    "\n",
    "    # ---- Early stopping logic ----\n",
    "    if val_loss + MIN_DELTA < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(model.state_dict(), best_path)\n",
    "        print(f\"  -> New best model saved (val_loss={val_loss:.6f})\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"  -> No improvement for {epochs_no_improve} epoch(s)\")\n",
    "\n",
    "        if epochs_no_improve >= PATIENCE:\n",
    "            print(f\"\\nEarly stopping triggered after {epoch} epochs.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40c213ce-f64d-4c0e-9d27-1a2e685289bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch shape : torch.Size([64, 60, 6])\n",
      "Preds batch shape : torch.Size([64, 1])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    xb, yb = next(iter(train_dl))\n",
    "    xb = xb.to(device)\n",
    "    preds = model(xb)\n",
    "    print(\"Input batch shape :\", xb.shape)      \n",
    "    print(\"Preds batch shape :\", preds.shape)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
