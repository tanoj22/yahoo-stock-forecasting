{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2aefc5ae-4989-4d8e-b968-c7539d52e650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root on sys.path: C:\\Users\\saita\\Yahoo Stock Forecasting\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path.cwd()\n",
    "if ROOT.name.lower() == \"notebooks\":  \n",
    "    ROOT = ROOT.parent\n",
    "\n",
    "sys.path.insert(0, str(ROOT))\n",
    "print(\"Project root on sys.path:\", ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ab4e1d1-941b-4393-a112-decb9b43ba93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from src.models_tft import TimeSeriesTransformer\n",
    "from src.config import MODEL_DIR, LSTM_BATCH_SIZE, LSTM_NUM_EPOCHS, LSTM_LR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f9a3ad2-3f19-48b0-826f-fa53a6656b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (1217, 60, 6)\n",
      "y_train: (1217, 1)\n",
      "X_val  : (213, 60, 6)\n",
      "y_val  : (213, 1)\n",
      "X_test : (215, 60, 6)\n",
      "y_test : (215, 1)\n",
      "Sequence length: 60 | Num features: 6\n"
     ]
    }
   ],
   "source": [
    "data = np.load(\"lstm_data.npz\")\n",
    "\n",
    "X_train = data[\"X_train\"]  # (N_train, seq_len, num_features)\n",
    "y_train = data[\"y_train\"]  # (N_train, 1)\n",
    "X_val   = data[\"X_val\"]\n",
    "y_val   = data[\"y_val\"]\n",
    "X_test  = data[\"X_test\"]\n",
    "y_test  = data[\"y_test\"]\n",
    "\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"X_val  :\", X_val.shape)\n",
    "print(\"y_val  :\", y_val.shape)\n",
    "print(\"X_test :\", X_test.shape)\n",
    "print(\"y_test :\", y_test.shape)\n",
    "\n",
    "num_features = X_train.shape[-1]\n",
    "seq_len = X_train.shape[1]\n",
    "print(\"Sequence length:\", seq_len, \"| Num features:\", num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26961586-4183-4585-9b70-596261ac81f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "train_ds = TensorDataset(\n",
    "    torch.tensor(X_train, dtype=torch.float32),\n",
    "    torch.tensor(y_train, dtype=torch.float32),\n",
    ")\n",
    "\n",
    "val_ds = TensorDataset(\n",
    "    torch.tensor(X_val, dtype=torch.float32),\n",
    "    torch.tensor(y_val, dtype=torch.float32),\n",
    ")\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=LSTM_BATCH_SIZE, shuffle=True)\n",
    "val_dl   = DataLoader(val_ds,   batch_size=LSTM_BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5f15796-885b-440d-b4ed-6c6e2c955f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeSeriesTransformer(\n",
      "  (input_proj): Linear(in_features=6, out_features=64, bias=True)\n",
      "  (pos_encoder): PositionalEncoding()\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=64, out_features=128, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=128, out_features=64, bias=True)\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc_out): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "Model will be saved to: C:\\Users\\saita\\Yahoo Stock Forecasting\\models\\tft_model.pth\n",
      "Early stopping: patience=7, min_delta=0.0001\n"
     ]
    }
   ],
   "source": [
    "# Transformer hyperparameters â€“ you can tweak\n",
    "D_MODEL = 64\n",
    "NHEAD = 4\n",
    "NUM_LAYERS = 2\n",
    "DIM_FEEDFORWARD = 128\n",
    "DROPOUT = 0.1\n",
    "\n",
    "model = TimeSeriesTransformer(\n",
    "    num_features=num_features,\n",
    "    d_model=D_MODEL,\n",
    "    nhead=NHEAD,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dim_feedforward=DIM_FEEDFORWARD,\n",
    "    dropout=DROPOUT,\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LSTM_LR)\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "best_path = os.path.join(MODEL_DIR, \"tft_model.pth\")\n",
    "\n",
    "# Early stopping params\n",
    "PATIENCE = 7\n",
    "MIN_DELTA = 1e-4\n",
    "\n",
    "print(model)\n",
    "print(\"Model will be saved to:\", best_path)\n",
    "print(f\"Early stopping: patience={PATIENCE}, min_delta={MIN_DELTA}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5c02a99-fe50-4778-929c-01f1b3e213e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | train_loss=0.081418 | val_loss=0.091790\n",
      "  -> New best model saved (val_loss=0.091790)\n",
      "Epoch 002 | train_loss=0.014715 | val_loss=0.012822\n",
      "  -> New best model saved (val_loss=0.012822)\n",
      "Epoch 003 | train_loss=0.008129 | val_loss=0.015312\n",
      "  -> No improvement for 1 epoch(s)\n",
      "Epoch 004 | train_loss=0.007851 | val_loss=0.015960\n",
      "  -> No improvement for 2 epoch(s)\n",
      "Epoch 005 | train_loss=0.006058 | val_loss=0.021678\n",
      "  -> No improvement for 3 epoch(s)\n",
      "Epoch 006 | train_loss=0.005742 | val_loss=0.004860\n",
      "  -> New best model saved (val_loss=0.004860)\n",
      "Epoch 007 | train_loss=0.005734 | val_loss=0.009761\n",
      "  -> No improvement for 1 epoch(s)\n",
      "Epoch 008 | train_loss=0.003503 | val_loss=0.010600\n",
      "  -> No improvement for 2 epoch(s)\n",
      "Epoch 009 | train_loss=0.002979 | val_loss=0.007575\n",
      "  -> No improvement for 3 epoch(s)\n",
      "Epoch 010 | train_loss=0.002621 | val_loss=0.006355\n",
      "  -> No improvement for 4 epoch(s)\n",
      "Epoch 011 | train_loss=0.002788 | val_loss=0.003180\n",
      "  -> New best model saved (val_loss=0.003180)\n",
      "Epoch 012 | train_loss=0.002908 | val_loss=0.006797\n",
      "  -> No improvement for 1 epoch(s)\n",
      "Epoch 013 | train_loss=0.002486 | val_loss=0.002540\n",
      "  -> New best model saved (val_loss=0.002540)\n",
      "Epoch 014 | train_loss=0.002172 | val_loss=0.010715\n",
      "  -> No improvement for 1 epoch(s)\n",
      "Epoch 015 | train_loss=0.002413 | val_loss=0.005894\n",
      "  -> No improvement for 2 epoch(s)\n",
      "Epoch 016 | train_loss=0.002223 | val_loss=0.002806\n",
      "  -> No improvement for 3 epoch(s)\n",
      "Epoch 017 | train_loss=0.001746 | val_loss=0.003273\n",
      "  -> No improvement for 4 epoch(s)\n",
      "Epoch 018 | train_loss=0.001592 | val_loss=0.005845\n",
      "  -> No improvement for 5 epoch(s)\n",
      "Epoch 019 | train_loss=0.001672 | val_loss=0.004669\n",
      "  -> No improvement for 6 epoch(s)\n",
      "Epoch 020 | train_loss=0.001598 | val_loss=0.002832\n",
      "  -> No improvement for 7 epoch(s)\n",
      "\n",
      "Early stopping triggered after 20 epochs.\n"
     ]
    }
   ],
   "source": [
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in range(1, LSTM_NUM_EPOCHS + 1):\n",
    "    # ---- Training ----\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for xb, yb in train_dl:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(xb)          # (batch, 1)\n",
    "        loss = criterion(preds, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * xb.size(0)\n",
    "\n",
    "    train_loss /= len(train_dl.dataset)\n",
    "\n",
    "    # ---- Validation ----\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_dl:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "\n",
    "            val_loss += loss.item() * xb.size(0)\n",
    "\n",
    "    val_loss /= len(val_dl.dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch:03d} | train_loss={train_loss:.6f} | val_loss={val_loss:.6f}\")\n",
    "\n",
    "    # ---- Early stopping logic ----\n",
    "    if val_loss + MIN_DELTA < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(model.state_dict(), best_path)\n",
    "        print(f\"  -> New best model saved (val_loss={val_loss:.6f})\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"  -> No improvement for {epochs_no_improve} epoch(s)\")\n",
    "\n",
    "        if epochs_no_improve >= PATIENCE:\n",
    "            print(f\"\\nEarly stopping triggered after {epoch} epochs.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9638b9ea-56c5-46f2-bfac-3db7dc61f1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch shape : torch.Size([64, 60, 6])\n",
      "Preds batch shape : torch.Size([64, 1])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    xb, yb = next(iter(train_dl))\n",
    "    xb = xb.to(device)\n",
    "    preds = model(xb)\n",
    "    print(\"Input batch shape :\", xb.shape)      \n",
    "    print(\"Preds batch shape :\", preds.shape)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (breastflex)",
   "language": "python",
   "name": "breastflex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
